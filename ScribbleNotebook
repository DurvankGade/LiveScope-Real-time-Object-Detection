{
  "metadata": {
    "name": "New JSNB",
    "language_info": {
      "name": "JavaScipt",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "<style>\n    body {\n            margin: 0;\n            height: 100vh;\n            display: flex;\n            flex-direction: column;\n            justify-content: center;\n            align-items: center;\n            background-image: \n                url('https://as1.ftcdn.net/v2/jpg/05/68/42/84/1000_F_568428476_UuKjs74nkS6nZoIa2cTVUjxrFhbTJ7a0.jpg'), /* Background image */\n                radial-gradient(circle, rgba(255, 255, 255, 0.1), rgba(0, 0, 0, 0.5)); /* Light overlay */\n            background-size: cover; /* Cover the entire viewport */\n            background-position: center; /* Center the image */\n            font-family: 'Courier New', Courier, monospace; /* Retro font */\n            color: #fff; /* Default text color */\n            text-align: center; /* Centering text */\n            position: relative; /* For layering */\n        }\n\n        h1 {\n            color: #ffcc00; /* Bright yellow */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5); /* Text shadow for depth */\n            margin: 10px 0; /* Margin for spacing */\n        }\n\n        h2 {\n            color: #ff6699; /* Bright pink */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h3 {\n            color: #66ffcc; /* Light teal */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h4 {\n            color: #ff9966; /* Light orange */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h5 {\n            color: #cc99ff; /* Light purple */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h6 {\n            color: #ffcc66; /* Light yellow-orange */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        p {\n            color: #ffffff; /* White for paragraphs */\n            margin: 10px 20px; /* Margin for spacing */\n            text-shadow: 1px 1px 0 rgba(0, 0, 0, 0.5); /* Slight shadow for paragraphs */\n        }\n\n        .wrapper {\n            display: flex;\n            overflow-x: auto; /* Allow horizontal scrolling */\n            padding: 20px;\n            position: relative; /* For overlay positioning */\n            z-index: 2; /* Ensure cards are above the overlay */\n        }\n\n        .item {\n            position: relative;\n            margin: 0 10px;\n            transition: transform 0.3s ease; /* Smooth transition for scaling */\n        }\n\n        .item img {\n            width: 300px; /* Fixed width for images */\n            border-radius: 10px; /* Rounded corners */\n            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5); /* Shadow for depth */\n        }\n\n        .item:hover {\n            transform: scale(1.1); /* Scale up on hover */\n        }\n\n        /* Move adjacent items when one is hovered */\n        .item:hover ~ .item {\n            transform: translateX(20px); /* Push right */\n        }\n\n        .item:hover ~ .item:hover {\n            transform: translateX(0); /* Reset for hovered item */\n        }\n\n        .item:hover ~ .item:hover ~ .item {\n            transform: translateX(20px); /* Push right for next item */\n        }\n\n        .item:hover ~ .item:hover ~ .item:hover {\n            transform: translateX(0); /* Reset for hovered item */\n        }</style>\n<h1>LiveScope</h1>",
      "status": "",
      "output": "<style>\n    body {\n            margin: 0;\n            height: 100vh;\n            display: flex;\n            flex-direction: column;\n            justify-content: center;\n            align-items: center;\n            background-image: \n                url('https://as1.ftcdn.net/v2/jpg/05/68/42/84/1000_F_568428476_UuKjs74nkS6nZoIa2cTVUjxrFhbTJ7a0.jpg'), /* Background image */\n                radial-gradient(circle, rgba(255, 255, 255, 0.1), rgba(0, 0, 0, 0.5)); /* Light overlay */\n            background-size: cover; /* Cover the entire viewport */\n            background-position: center; /* Center the image */\n            font-family: 'Courier New', Courier, monospace; /* Retro font */\n            color: #fff; /* Default text color */\n            text-align: center; /* Centering text */\n            position: relative; /* For layering */\n        }\n\n        h1 {\n            color: #ffcc00; /* Bright yellow */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5); /* Text shadow for depth */\n            margin: 10px 0; /* Margin for spacing */\n        }\n\n        h2 {\n            color: #ff6699; /* Bright pink */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h3 {\n            color: #66ffcc; /* Light teal */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h4 {\n            color: #ff9966; /* Light orange */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h5 {\n            color: #cc99ff; /* Light purple */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        h6 {\n            color: #ffcc66; /* Light yellow-orange */\n            text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.5);\n            margin: 10px 0;\n        }\n\n        p {\n            color: #ffffff; /* White for paragraphs */\n            margin: 10px 20px; /* Margin for spacing */\n            text-shadow: 1px 1px 0 rgba(0, 0, 0, 0.5); /* Slight shadow for paragraphs */\n        }\n\n        .wrapper {\n            display: flex;\n            overflow-x: auto; /* Allow horizontal scrolling */\n            padding: 20px;\n            position: relative; /* For overlay positioning */\n            z-index: 2; /* Ensure cards are above the overlay */\n        }\n\n        .item {\n            position: relative;\n            margin: 0 10px;\n            transition: transform 0.3s ease; /* Smooth transition for scaling */\n        }\n\n        .item img {\n            width: 300px; /* Fixed width for images */\n            border-radius: 10px; /* Rounded corners */\n            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5); /* Shadow for depth */\n        }\n\n        .item:hover {\n            transform: scale(1.1); /* Scale up on hover */\n        }\n\n        /* Move adjacent items when one is hovered */\n        .item:hover ~ .item {\n            transform: translateX(20px); /* Push right */\n        }\n\n        .item:hover ~ .item:hover {\n            transform: translateX(0); /* Reset for hovered item */\n        }\n\n        .item:hover ~ .item:hover ~ .item {\n            transform: translateX(20px); /* Push right for next item */\n        }\n\n        .item:hover ~ .item:hover ~ .item:hover {\n            transform: translateX(0); /* Reset for hovered item */\n        }</style>\n<h1>LiveScope</h1>",
      "type": ""
    },
    {
      "code": "<h5>Welcome to LiveScope, a cutting-edge project designed to perform real-time object detection and recognition using JavaScript and TensorFlow.js. This project leverages the power of machine learning to identify objects in videos, making it perfect for various applications like surveillance, autonomous vehicles, and more.<h5>",
      "status": "",
      "output": "<h5>Welcome to LiveScope, a cutting-edge project designed to perform real-time object detection and recognition using JavaScript and TensorFlow.js. This project leverages the power of machine learning to identify objects in videos, making it perfect for various applications like surveillance, autonomous vehicles, and more.</h5><h5></h5>",
      "type": "html"
    },
    {
      "code": "<h3>What we can do:</h3><h6>\n1.Real-Time Object Detection and Recognition using a video file<br>\n2.Attention Detection<br></h6>",
      "status": "",
      "output": "<h3>What we can do:</h3><h6>\n1.Real-Time Object Detection and Recognition using a video file<br>\n2.Attention Detection<br></h6>",
      "type": "html"
    },
    {
      "code": "<h3>1.Real-Time Object Detection and Recognition</h3>\n",
      "status": "",
      "output": "<h3>1.Real-Time Object Detection and Recognition</h3>\n",
      "type": "html"
    },
    {
      "code": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Object Detection with COCO-SSD</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\">\n    <style>\n        body {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n        }\n\n        header {\n            background: #007bff;\n            color: white;\n            padding: 10px;\n            text-align: center;\n            width: 100%;\n        }\n\n        section {\n            margin: 20px;\n            max-width: 800px;\n            background: white;\n            border-radius: 8px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n            padding: 20px;\n        }\n\n        img {\n            border: 1px solid #ccc;\n            margin-bottom: 10px;\n            border-radius: 4px;\n        }\n\n        button {\n            padding: 10px 20px;\n            font-size: 16px;\n            background: #28a745;\n            border: none;\n            color: white;\n            cursor: pointer;\n            border-radius: 4px;\n            transition: background 0.3s;\n        }\n\n        footer {\n            margin-top: auto;\n            padding: 20px;\n            background: #007bff;\n            color: white;\n            text-align: center;\n            width: 100%;\n        }\n\n        .modal {\n            display: none;\n            position: fixed;\n            z-index: 1000;\n            left: 0;\n            top: 0;\n            width: 100%;\n            height: 100%;\n            background-color: rgba(0, 0, 0, 0.5);\n            justify-content: center;\n            align-items: center;\n        }\n\n        .modal-content {\n            background-color: white;\n            padding: 20px;\n            border-radius: 8px;\n            width: 400px;\n        }\n\n        .close {\n            cursor: pointer;\n            float: right;\n            color: #aaa;\n            font-size: 20px;\n        }\n    </style>\n</head>\n<body>\n    <header>\n        <h1>Object Detection using COCO-SSD</h1>\n    </header>\n\n    <section>\n        <h2>Application Overview</h2>\n        <p>This application leverages TensorFlow.js and the COCO-SSD model to detect and label objects in real-time.</p>\n        <button onclick=\"document.getElementById('modal').style.display='flex'\">How It Works <i class=\"fas fa-info-circle\"></i></button>\n    </section>\n\n    <section>\n        <h2>Demo Image</h2>\n        <a href=\"https://ibb.co/DG7Fc5N\">\n            <img src=\"https://i.ibb.co/hMJLzfC/Screenshot-2024-11-10-114333.png\" alt=\"Screenshot-2024-11-10-114333\" border=\"0\">\n        </a>\n       \n        <p id=\"waitingMessage\">Please wait while the model is loading...</p>\n    </section>\n\n    <footer>\n        <p>&copy; 2024 Object Detection App</p>\n    </footer>\n\n    <!-- Modal for Detailed Working Steps -->\n    <div id=\"modal\" class=\"modal\" onclick=\"this.style.display='none'\">\n        <div class=\"modal-content\" onclick=\"event.stopPropagation()\">\n            <span class=\"close\" onclick=\"document.getElementById('modal').style.display='none'\">&times;</span>\n            <h2>How This Application Works</h2>\n            <ol>\n                <li><strong>Setup:</strong> The application loads TensorFlow.js and COCO-SSD model scripts.</li>\n                <li><strong>Webcam Access:</strong> (Not applicable in this demo)</li>\n                <li><strong>Model Loading:</strong> The pre-trained COCO-SSD model is loaded for object recognition.</li>\n                <li><strong>Detection Process:</strong> The application is ready to detect objects in images.</li>\n                <li><strong>Real-Time Results:</strong> Detected objects can be outlined on images if a real-time feed were included.</li>\n            </ol>\n        </div>\n    </div>\n\n    <script>\n        // Note: Detection script is removed as webcam and detection aren't utilized in this version.\n    </script>\n</body>\n</html>",
      "status": "",
      "output": "\n\n\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Object Detection with COCO-SSD</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\">\n    <style>\n        body {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n        }\n\n        header {\n            background: #007bff;\n            color: white;\n            padding: 10px;\n            text-align: center;\n            width: 100%;\n        }\n\n        section {\n            margin: 20px;\n            max-width: 800px;\n            background: white;\n            border-radius: 8px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n            padding: 20px;\n        }\n\n        img {\n            border: 1px solid #ccc;\n            margin-bottom: 10px;\n            border-radius: 4px;\n        }\n\n        button {\n            padding: 10px 20px;\n            font-size: 16px;\n            background: #28a745;\n            border: none;\n            color: white;\n            cursor: pointer;\n            border-radius: 4px;\n            transition: background 0.3s;\n        }\n\n        footer {\n            margin-top: auto;\n            padding: 20px;\n            background: #007bff;\n            color: white;\n            text-align: center;\n            width: 100%;\n        }\n\n        .modal {\n            display: none;\n            position: fixed;\n            z-index: 1000;\n            left: 0;\n            top: 0;\n            width: 100%;\n            height: 100%;\n            background-color: rgba(0, 0, 0, 0.5);\n            justify-content: center;\n            align-items: center;\n        }\n\n        .modal-content {\n            background-color: white;\n            padding: 20px;\n            border-radius: 8px;\n            width: 400px;\n        }\n\n        .close {\n            cursor: pointer;\n            float: right;\n            color: #aaa;\n            font-size: 20px;\n        }\n    </style>\n\n\n    <header>\n        <h1>Object Detection using COCO-SSD</h1>\n    </header>\n\n    <section>\n        <h2>Application Overview</h2>\n        <p>This application leverages TensorFlow.js and the COCO-SSD model to detect and label objects in real-time.</p>\n        <button onclick=\"document.getElementById('modal').style.display='flex'\">How It Works <i class=\"fas fa-info-circle\"></i></button>\n    </section>\n\n    <section>\n        <h2>Demo Image</h2>\n        <a href=\"https://ibb.co/DG7Fc5N\">\n            <img src=\"https://i.ibb.co/hMJLzfC/Screenshot-2024-11-10-114333.png\" alt=\"Screenshot-2024-11-10-114333\" border=\"0\">\n        </a>\n       \n        <p id=\"waitingMessage\">Please wait while the model is loading...</p>\n    </section>\n\n    <footer>\n        <p>© 2024 Object Detection App</p>\n    </footer>\n\n    <!-- Modal for Detailed Working Steps -->\n    <div id=\"modal\" class=\"modal\" onclick=\"this.style.display='none'\">\n        <div class=\"modal-content\" onclick=\"event.stopPropagation()\">\n            <span class=\"close\" onclick=\"document.getElementById('modal').style.display='none'\">×</span>\n            <h2>How This Application Works</h2>\n            <ol>\n                <li><strong>Setup:</strong> The application loads TensorFlow.js and COCO-SSD model scripts.</li>\n                <li><strong>Webcam Access:</strong> (Not applicable in this demo)</li>\n                <li><strong>Model Loading:</strong> The pre-trained COCO-SSD model is loaded for object recognition.</li>\n                <li><strong>Detection Process:</strong> The application is ready to detect objects in images.</li>\n                <li><strong>Real-Time Results:</strong> Detected objects can be outlined on images if a real-time feed were included.</li>\n            </ol>\n        </div>\n    </div>\n\n    <script>\n        // Note: Detection script is removed as webcam and detection aren't utilized in this version.\n    </script>\n\n",
      "type": "html"
    },
    {
      "code": "(async () => {\n    const container = document.createElement('div');\n    container.style.display = 'flex';\n    container.style.flexDirection = 'column';\n    container.style.alignItems = 'center';\n    document.body.appendChild(container);\n\n    const waitingMessage = document.createElement('p');\n    waitingMessage.innerText = 'Setting up... Please wait.';\n    container.appendChild(waitingMessage);\n\n    if(scrib.isSandboxed()) {\n        alert(\"Please take it out of sandbox by clicking the red button on top right corner\");\n        return;\n    }\n\n    const loadScript = (url) => {\n        return new Promise((resolve, reject) => {\n            const script = document.createElement('script');\n            script.src = url;\n            script.onload = resolve;\n            script.onerror = (e) => reject(new Error(`Failed to load script: ${url}`));\n            document.head.appendChild(script);\n        });\n    };\n\n    await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs').catch(console.error);\n    await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd').catch(console.error);\n\n    const video = document.createElement('video');\n    video.width = 640;\n    video.height = 480;\n    video.id = 'myVideo';\n    video.autoplay = true;\n    container.appendChild(video);\n\n    let stream;\n    navigator.mediaDevices.getUserMedia({ video: true })\n        .then((cameraStream) => {\n            stream = cameraStream;\n            video.srcObject = cameraStream;\n        })\n        .catch((error) => {\n            console.error('Error accessing camera:', error.message);\n        });\n\n    const model = await cocoSsd.load().catch(console.error);\n\n    const canvas = document.createElement('canvas');\n    canvas.width = video.width;\n    canvas.height = video.height;\n    canvas.id = 'canvas';\n    container.appendChild(canvas);\n    const ctx = canvas.getContext('2d');\n\n    waitingMessage.style.display = 'none';\n\n    const analyzeButton = document.createElement('button');\n    analyzeButton.innerText = 'Analyze';\n    container.insertBefore(analyzeButton, video);\n\n    const colorMap = {\n        'person': 'red', 'bicycle': 'blue', 'car': 'green', 'motorcycle': 'yellow', 'airplane': 'purple',\n        'bus': 'orange', 'train': 'pink', 'truck': 'brown', 'boat': 'cyan', 'traffic light': 'magenta',\n        'fire hydrant': 'lime', 'stop sign': 'indigo', 'parking meter': 'violet', 'bench': 'gold',\n        'bird': 'teal', 'cat': 'olive', 'dog': 'maroon', 'horse': 'navy', 'sheep': 'aqua', 'cow': 'lime',\n        'elephant': 'fuchsia', 'bear': 'salmon', 'zebra': 'khaki', 'giraffe': 'coral', 'backpack': 'orchid',\n        'umbrella': 'plum', 'handbag': 'silver', 'tie': 'lavender', 'suitcase': 'peachpuff', 'frisbee': 'hotpink',\n        'skis': 'firebrick', 'snowboard': 'crimson', 'sports ball': 'greenyellow', 'kite': 'turquoise',\n        'baseball bat': 'mediumslateblue', 'baseball glove': 'dodgerblue', 'skateboard': 'seashell', 'surfboard': 'tan',\n        'tennis racket': 'lightcoral', 'bottle': 'palegoldenrod', 'wine glass': 'tomato', 'cup': 'peru',\n        'fork': 'slateblue', 'knife': 'mediumvioletred', 'spoon': 'mediumspringgreen', 'bowl': 'midnightblue',\n        'banana': 'lightgreen', 'apple': 'sandybrown', 'sandwich': 'lemonchiffon', 'orange': 'chocolate',\n        'broccoli': 'springgreen', 'carrot': 'darkseagreen', 'hot dog': 'indianred', 'pizza': 'peru',\n        'donut': 'goldenrod', 'cake': 'mediumseagreen', 'chair': 'powderblue', 'couch': 'sienna', 'potted plant': 'coral',\n        'bed': 'forestgreen', 'dining table': 'steelblue', 'toilet': 'gainsboro', 'tv': 'burlywood', 'laptop': 'mistyrose',\n        'mouse': 'beige', 'remote': 'azure', 'keyboard': 'thistle', 'cell phone': 'gold', 'microwave': 'lightsteelblue',\n        'oven': 'chartreuse', 'toaster': 'blanchedalmond', 'sink': 'darkorchid', 'refrigerator': 'palegreen',\n        'book': 'darkgoldenrod', 'clock': 'lightskyblue', 'vase': 'darkblue', 'scissors': 'darkslateblue',\n        'teddy bear': 'saddlebrown', 'hair drier': 'limegreen', 'toothbrush': 'slategray'\n    };\n\n    async function detectObjects() {\n        if (video.readyState === 4) {\n            try {\n                const predictions = await model.detect(video);\n                ctx.clearRect(0, 0, canvas.width, canvas.height);\n                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n\n                predictions.forEach(prediction => {\n                    ctx.beginPath();\n                    ctx.rect(...prediction.bbox);\n                    ctx.lineWidth = 4;\n                    ctx.strokeStyle = colorMap[prediction.class] || 'red';\n                    ctx.fillStyle = colorMap[prediction.class] || 'red';\n                    ctx.stroke();\n                    ctx.font = '16px Arial';\n                    ctx.fillText(\n                        `${prediction.class} - ${Math.round(prediction.score * 100)}%`,\n                        prediction.bbox[0],\n                        prediction.bbox[1] > 20 ? prediction.bbox[1] - 10 : 20\n                    );\n                });\n            } catch (error) {\n                console.error(\"Detection error: \", error);\n            }\n        }\n        requestAnimationFrame(detectObjects);\n    }\n\n    analyzeButton.onclick = () => {\n        detectObjects();\n        console.log(\"Analyze button clicked, starting real-time detection\");\n    };\n})();\n\n",
      "status": "[2]<br><span style=\"font-size:8px\">3.319s<span></span></span>",
      "output": "",
      "type": "code"
    },
    {
      "code": "<h3>2.Attention Detection</h3>",
      "status": "",
      "output": "<h3>2.Attention Detection</h3>",
      "type": "html"
    },
    {
      "code": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Attention Detection Application</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\">\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n           \n            margin: 0;\n            padding: 0;\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            color: #333;\n        }\n\n        header {\n            background: #007bff;\n            color: white;\n            width: 100%;\n            padding: 20px;\n            text-align: center;\n            position: relative;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n        }\n\n        h1 {\n            margin: 0;\n            font-size: 2.5em;\n        }\n\n        section {\n            background: white;\n            margin: 20px;\n            max-width: 800px;\n            border-radius: 10px;\n            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);\n            overflow: hidden;\n            transition: transform 0.3s;\n            padding: 20px;\n        }\n\n        section:hover {\n            transform: translateY(-5px);\n        }\n\n        h2 {\n            font-size: 1.8em;\n            margin-bottom: 10px;\n            color: #007bff;\n            position: relative;\n        }\n\n        h2::after {\n            content: '';\n            position: absolute;\n            width: 50px;\n            height: 5px;\n            background: #007bff;\n            bottom: -5px;\n            left: 0;\n        }\n\n        p {\n            line-height: 1.6;\n            margin: 10px 0;\n        }\n\n        .steps {\n            list-style-type: none;\n            padding: 0;\n        }\n\n        .steps li {\n            background: #e9ecef;\n            margin: 10px 0;\n            padding: 15px;\n            border-radius: 8px;\n            position: relative;\n            transition: transform 0.2s;\n            cursor: pointer;\n        }\n\n        .steps li:hover {\n            transform: translateY(-2px);\n        }\n\n        .steps li:before {\n            content: counter(step);\n            counter-increment: step;\n            background: #28a745;\n            color: white;\n            border-radius: 50%;\n            width: 30px;\n            height: 30px;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            position: absolute;\n            left: -40px;\n            top: 15px;\n            font-weight: bold;\n        }\n\n        footer {\n            background: #007bff;\n            color: white;\n            padding: 20px;\n            text-align: center;\n            width: 100%;\n            margin-top: auto;\n        }\n\n        .demo-image {\n            display: flex;\n            justify-content: center;\n            margin: 20px 0;\n        }\n\n        .demo-image img {\n            max-width: 100%;\n            border-radius: 8px;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n        }\n\n        @media screen and (max-width: 768px) {\n            h2 {\n                font-size: 1.5em;\n            }\n\n            section {\n                margin: 10px;\n                padding: 10px;\n            }\n        }\n    </style>\n</head>\n<body>\n\n<header>\n    <h1>Attention Detection Application</h1>\n</header>\n\n<section>\n    <h2>What is Attention Detection?</h2>\n    <p>\n        Attention Detection utilizes computer vision to monitor and assess an individual's attention level based on face\n        detection through the camera. This technology can be applied in various fields, from educational environments to\n        workplace productivity. This application leverages TensorFlow.js and BlazeFace model for real-time face\n        detection.\n    </p>\n</section>\n\n<section>\n    <h2>Steps to Implement Attention Detection</h2>\n    <ol class=\"steps\">\n        <li>Load the required TensorFlow.js and BlazeFace scripts.</li>\n        <li>Create a video element to stream the user's webcam.</li>\n        <li>Set up a canvas element to overlay face detection results.</li>\n        <li>Initialize the webcam stream and present it to the video element.</li>\n        <li>Load the BlazeFace model for face detection capabilities.</li>\n        <li>In a loop, detect faces from the video stream, drawing overlays on the canvas.</li>\n        <li>Evaluate if the detected face indicates high attention based on position and size.</li>\n    </ol>\n</section>\n\n<section>\n    <h2>Applications of Attention Detection</h2>\n    <p>\n        Attention Detection can be useful in various scenarios:\n    </p>\n    <ul>\n        <li><strong>Education:</strong> Track students' focus during online learning sessions.</li>\n        <li><strong>Meetings:</strong> Assess engagement levels in virtual meetings.</li>\n        <li><strong>Safety:</strong> Monitor driver attentiveness in transportation.</li>\n        <li><strong>Marketing:</strong> Evaluate consumer attention towards advertisements.</li>\n    </ul>\n\n    <div class=\"demo-image\">\n        <a href=\"https://ibb.co/GHXHKzt\">\n            <img src=\"https://i.ibb.co/MBbBYFG/Screenshot-2024-11-10-125957.png\" alt=\"Attention Detection Demo\" border=\"0\">\n        </a>\n    </div>\n</section>\n\n<footer>\n    <p>&copy; 2024 Attention Detection App</p>\n</footer>\n\n<script>\n    (async () => {\n        await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs');\n        await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface');\n\n        function loadScript(url) {\n            return new Promise((resolve, reject) => {\n                const script = document.createElement('script');\n                script.src = url;\n                script.onload = resolve;\n                script.onerror = reject;\n                document.head.appendChild(script);\n            });\n        }\n\n        const body = document.body;\n\n        const video = document.createElement('video');\n        video.id = 'video';\n        video.width = 640;\n        video.height = 480;\n        video.autoplay = true;\n        body.appendChild(video);\n\n        const canvas = document.createElement('canvas');\n        canvas.id = 'overlay';\n        canvas.width = 640;\n        canvas.height = 480;\n        canvas.style.position = 'absolute';\n        canvas.style.top = '0';\n        canvas.style.left = '0';\n        body.appendChild(canvas);\n\n        const attentionStatus = document.createElement('p');\n        attentionStatus.id = 'status';\n        attentionStatus.textContent = 'Attention Status: Unknown';\n        body.appendChild(attentionStatus);\n\n        const videoElement = document.getElementById('video');\n        const canvasElement = document.getElementById('overlay');\n        const statusElement = document.getElementById('status');\n        const ctx = canvasElement.getContext('2d');\n\n        try {\n            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n            videoElement.srcObject = stream;\n        } catch (error) {\n            console.error('Error accessing camera:', error);\n            alert('Error accessing camera. Please ensure you have allowed permissions.');\n            return;\n        }\n\n        const model = await blazeface.load();\n\n        async function detectAttention() {\n            const returnTensors = false;\n            const predictions = await model.estimateFaces(videoElement, returnTensors);\n\n            ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n\n            if (predictions.length > 0) {\n                predictions.forEach(prediction => {\n                    const start = prediction.topLeft;\n                    const end = prediction.bottomRight;\n                    const size = [end[0] - start[0], end[1] - start[1]];\n\n                    ctx.fillStyle = 'rgba(0, 255, 0, 0.5)';\n                    ctx.fillRect(start[0], start[1], size[0], size[1]);\n\n                    const headCenterX = (start[0] + end[0]) / 2;\n                    const headCenterY = (start[1] + end[1]) / 2;\n                    const faceWidth = end[0] - start[0];\n\n                    if (headCenterX > canvasElement.width * 0.35 && headCenterX < canvasElement.width * 0.65 && faceWidth > canvasElement.width * 0.2) {\n                        statusElement.textContent = 'Attention Status: Paying Attention';\n                    } else {\n                        statusElement.textContent = 'Attention Status: Not Paying Attention';\n                    }\n                });\n            } else {\n                statusElement.textContent = 'Attention Status: No Face Detected';\n            }\n            requestAnimationFrame(detectAttention);\n        }\n\n        detectAttention();\n    })();\n</script>\n\n</body>\n</html>",
      "status": "",
      "output": "\n\n\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Attention Detection Application</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\">\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n           \n            margin: 0;\n            padding: 0;\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            color: #333;\n        }\n\n        header {\n            background: #007bff;\n            color: white;\n            width: 100%;\n            padding: 20px;\n            text-align: center;\n            position: relative;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n        }\n\n        h1 {\n            margin: 0;\n            font-size: 2.5em;\n        }\n\n        section {\n            background: white;\n            margin: 20px;\n            max-width: 800px;\n            border-radius: 10px;\n            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);\n            overflow: hidden;\n            transition: transform 0.3s;\n            padding: 20px;\n        }\n\n        section:hover {\n            transform: translateY(-5px);\n        }\n\n        h2 {\n            font-size: 1.8em;\n            margin-bottom: 10px;\n            color: #007bff;\n            position: relative;\n        }\n\n        h2::after {\n            content: '';\n            position: absolute;\n            width: 50px;\n            height: 5px;\n            background: #007bff;\n            bottom: -5px;\n            left: 0;\n        }\n\n        p {\n            line-height: 1.6;\n            margin: 10px 0;\n        }\n\n        .steps {\n            list-style-type: none;\n            padding: 0;\n        }\n\n        .steps li {\n            background: #e9ecef;\n            margin: 10px 0;\n            padding: 15px;\n            border-radius: 8px;\n            position: relative;\n            transition: transform 0.2s;\n            cursor: pointer;\n        }\n\n        .steps li:hover {\n            transform: translateY(-2px);\n        }\n\n        .steps li:before {\n            content: counter(step);\n            counter-increment: step;\n            background: #28a745;\n            color: white;\n            border-radius: 50%;\n            width: 30px;\n            height: 30px;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            position: absolute;\n            left: -40px;\n            top: 15px;\n            font-weight: bold;\n        }\n\n        footer {\n            background: #007bff;\n            color: white;\n            padding: 20px;\n            text-align: center;\n            width: 100%;\n            margin-top: auto;\n        }\n\n        .demo-image {\n            display: flex;\n            justify-content: center;\n            margin: 20px 0;\n        }\n\n        .demo-image img {\n            max-width: 100%;\n            border-radius: 8px;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n        }\n\n        @media screen and (max-width: 768px) {\n            h2 {\n                font-size: 1.5em;\n            }\n\n            section {\n                margin: 10px;\n                padding: 10px;\n            }\n        }\n    </style>\n\n\n\n<header>\n    <h1>Attention Detection Application</h1>\n</header>\n\n<section>\n    <h2>What is Attention Detection?</h2>\n    <p>\n        Attention Detection utilizes computer vision to monitor and assess an individual's attention level based on face\n        detection through the camera. This technology can be applied in various fields, from educational environments to\n        workplace productivity. This application leverages TensorFlow.js and BlazeFace model for real-time face\n        detection.\n    </p>\n</section>\n\n<section>\n    <h2>Steps to Implement Attention Detection</h2>\n    <ol class=\"steps\">\n        <li>Load the required TensorFlow.js and BlazeFace scripts.</li>\n        <li>Create a video element to stream the user's webcam.</li>\n        <li>Set up a canvas element to overlay face detection results.</li>\n        <li>Initialize the webcam stream and present it to the video element.</li>\n        <li>Load the BlazeFace model for face detection capabilities.</li>\n        <li>In a loop, detect faces from the video stream, drawing overlays on the canvas.</li>\n        <li>Evaluate if the detected face indicates high attention based on position and size.</li>\n    </ol>\n</section>\n\n<section>\n    <h2>Applications of Attention Detection</h2>\n    <p>\n        Attention Detection can be useful in various scenarios:\n    </p>\n    <ul>\n        <li><strong>Education:</strong> Track students' focus during online learning sessions.</li>\n        <li><strong>Meetings:</strong> Assess engagement levels in virtual meetings.</li>\n        <li><strong>Safety:</strong> Monitor driver attentiveness in transportation.</li>\n        <li><strong>Marketing:</strong> Evaluate consumer attention towards advertisements.</li>\n    </ul>\n\n    <div class=\"demo-image\">\n        <a href=\"https://ibb.co/GHXHKzt\">\n            <img src=\"https://i.ibb.co/MBbBYFG/Screenshot-2024-11-10-125957.png\" alt=\"Attention Detection Demo\" border=\"0\">\n        </a>\n    </div>\n</section>\n\n<footer>\n    <p>© 2024 Attention Detection App</p>\n</footer>\n\n<script>\n    (async () => {\n        await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs');\n        await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface');\n\n        function loadScript(url) {\n            return new Promise((resolve, reject) => {\n                const script = document.createElement('script');\n                script.src = url;\n                script.onload = resolve;\n                script.onerror = reject;\n                document.head.appendChild(script);\n            });\n        }\n\n        const body = document.body;\n\n        const video = document.createElement('video');\n        video.id = 'video';\n        video.width = 640;\n        video.height = 480;\n        video.autoplay = true;\n        body.appendChild(video);\n\n        const canvas = document.createElement('canvas');\n        canvas.id = 'overlay';\n        canvas.width = 640;\n        canvas.height = 480;\n        canvas.style.position = 'absolute';\n        canvas.style.top = '0';\n        canvas.style.left = '0';\n        body.appendChild(canvas);\n\n        const attentionStatus = document.createElement('p');\n        attentionStatus.id = 'status';\n        attentionStatus.textContent = 'Attention Status: Unknown';\n        body.appendChild(attentionStatus);\n\n        const videoElement = document.getElementById('video');\n        const canvasElement = document.getElementById('overlay');\n        const statusElement = document.getElementById('status');\n        const ctx = canvasElement.getContext('2d');\n\n        try {\n            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n            videoElement.srcObject = stream;\n        } catch (error) {\n            console.error('Error accessing camera:', error);\n            alert('Error accessing camera. Please ensure you have allowed permissions.');\n            return;\n        }\n\n        const model = await blazeface.load();\n\n        async function detectAttention() {\n            const returnTensors = false;\n            const predictions = await model.estimateFaces(videoElement, returnTensors);\n\n            ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n\n            if (predictions.length > 0) {\n                predictions.forEach(prediction => {\n                    const start = prediction.topLeft;\n                    const end = prediction.bottomRight;\n                    const size = [end[0] - start[0], end[1] - start[1]];\n\n                    ctx.fillStyle = 'rgba(0, 255, 0, 0.5)';\n                    ctx.fillRect(start[0], start[1], size[0], size[1]);\n\n                    const headCenterX = (start[0] + end[0]) / 2;\n                    const headCenterY = (start[1] + end[1]) / 2;\n                    const faceWidth = end[0] - start[0];\n\n                    if (headCenterX > canvasElement.width * 0.35 && headCenterX < canvasElement.width * 0.65 && faceWidth > canvasElement.width * 0.2) {\n                        statusElement.textContent = 'Attention Status: Paying Attention';\n                    } else {\n                        statusElement.textContent = 'Attention Status: Not Paying Attention';\n                    }\n                });\n            } else {\n                statusElement.textContent = 'Attention Status: No Face Detected';\n            }\n            requestAnimationFrame(detectAttention);\n        }\n\n        detectAttention();\n    })();\n</script>\n\n\n",
      "type": "html"
    },
    {
      "code": "(async () => {\n    await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs');\n    await loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface');\n\n    function loadScript(url) {\n        return new Promise((resolve, reject) => {\n            const script = document.createElement('script');\n            script.src = url;\n            script.onload = resolve;\n            script.onerror = reject;\n            document.head.appendChild(script);\n        });\n    }\n\n    const body = document.body;\n\n    const h1 = document.createElement('h1');\n    h1.textContent = 'Attention Detection';\n    body.appendChild(h1);\n\n    const video = document.createElement('video');\n    video.id = 'video';\n    video.width = 640;\n    video.height = 480;\n    video.autoplay = true;\n    body.appendChild(video);\n\n    const canvas = document.createElement('canvas');\n    canvas.id = 'overlay';\n    canvas.width = 640;\n    canvas.height = 480;\n    canvas.style.position = 'absolute';\n    canvas.style.top = '0';\n    canvas.style.left = '0';\n    body.appendChild(canvas);\n\n    const attentionStatus = document.createElement('p');\n    attentionStatus.id = 'status';\n    attentionStatus.textContent = 'Attention Status: Unknown';\n    body.appendChild(attentionStatus);\n\n    const videoElement = document.getElementById('video');\n    const canvasElement = document.getElementById('overlay');\n    const statusElement = document.getElementById('status');\n    const ctx = canvasElement.getContext('2d');\n\n    try {\n        const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n        videoElement.srcObject = stream;\n    } catch (error) {\n        console.error('Error accessing camera:', error);\n        alert('Error accessing camera. Please ensure you have allowed permissions.');\n        return;\n    }\n\n    const model = await blazeface.load();\n\n    async function detectAttention() {\n        const returnTensors = false;\n        const predictions = await model.estimateFaces(videoElement, returnTensors);\n\n        ctx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n\n        if (predictions.length > 0) {\n            predictions.forEach(prediction => {\n                const start = prediction.topLeft;\n                const end = prediction.bottomRight;\n                const size = [end[0] - start[0], end[1] - start[1]];\n\n                ctx.fillStyle = 'rgba(0, 255, 0, 0.5)';\n                ctx.fillRect(start[0], start[1], size[0], size[1]);\n\n                const headCenterX = (start[0] + end[0]) / 2;\n                const headCenterY = (start[1] + end[1]) / 2;\n                const faceWidth = end[0] - start[0];\n\n                if (headCenterX > canvasElement.width * 0.35 && headCenterX < canvasElement.width * 0.65 && faceWidth > canvasElement.width * 0.2) {\n                    statusElement.textContent = 'Attention Status: Paying Attention';\n                } else {\n                    statusElement.textContent = 'Attention Status: Not Paying Attention';\n                }\n            });\n        } else {\n            statusElement.textContent = 'Attention Status: No Face Detected';\n        }\n        requestAnimationFrame(detectAttention);\n    }\n\n    detectAttention();\n})();\n",
      "status": "[1]<br><span style=\"font-size:8px\">5.489s<span></span></span>",
      "output": "",
      "type": "code"
    }
  ],
  "source": "https://github.com/gopi-suvanam/jsnb",
  "run_on_load": false
}